{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory:  c:\\Users\\gheith\\OneDrive - ETS\\0 2023 MTI 865 - Apprentissage profind pour la vision par ordinateur\\CleanedGithub\\MTI865-Competition\\Gheith\\Cross_Teaching_CNN_CNN_plus\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "script_path = \"\"\n",
    "try:\n",
    "    os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    for root, dirs, files in os.walk(os.getcwd()):\n",
    "        # Skip 'data' directory and its subdirectories\n",
    "        if \"Data\" in dirs:\n",
    "            dirs.remove(\"Data\")\n",
    "\n",
    "        if \"mainSegmentationChallenge.ipynb\" in files:\n",
    "            script_path = root\n",
    "            break\n",
    "\n",
    "if script_path == \"\":\n",
    "    raise FileNotFoundError(\n",
    "        \"There is a problem in the folder structure.\\nCONTACT gheith.abinader@icloud.com (514)699-5611\"\n",
    "    )\n",
    "\n",
    "os.chdir(script_path)\n",
    "\n",
    "print(\"Current Working Directory: \", os.getcwd())\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e976350-3fd5-4406-8511-86a06a9b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220c7dcc-8438-454d-97b1-8e989a7b8f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put outside of the function for pickeling\n",
    "def worker_init_fn(worker_id):\n",
    "    random.seed(1208 + worker_id)\n",
    "\n",
    "\n",
    "def runTraining():\n",
    "    print(\"-\" * 40)\n",
    "    print(\"~~~~~~~~  Starting the training... ~~~~~~\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 16\n",
    "    secondaty_batch_size = 8\n",
    "    batch_size_val = 24\n",
    "    base_lr = 0.01  # Learning Rate\n",
    "    max_iterations = 30000\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    mask_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset(\n",
    "        \"train\",\n",
    "        transform=transform,\n",
    "        mask_transform=mask_transform,\n",
    "        augment=False,\n",
    "        equalize=False,\n",
    "    )\n",
    "\n",
    "    total_slices = len(train_set_full)\n",
    "    labeled_slice = 204\n",
    "    print(\n",
    "        \"Total silices is: {}, labeled slices is: {}\".format(\n",
    "            total_slices, labeled_slice\n",
    "        )\n",
    "    )\n",
    "    labeled_idxs = list(range(0, labeled_slice))\n",
    "    unlabeled_idxs = list(range(labeled_slice, total_slices))\n",
    "    batch_sampler = medicalDataLoader.TwoStreamBatchSampler(\n",
    "        labeled_idxs, unlabeled_idxs, batch_size, secondaty_batch_size\n",
    "    )\n",
    "    trainloader = DataLoader(train_set_full, batch_sampler=batch_sampler, num_workers=0)\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset(\n",
    "        \"val\", transform=transform, mask_transform=mask_transform, equalize=False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size_val,\n",
    "        worker_init_fn=np.random.seed(0),\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName1, modelName2 = \"K3\", \"K5\"\n",
    "    print(\" Model Name1: {}\".format(modelName1))\n",
    "    print(\" Model Name2: {}\".format(modelName2))\n",
    "\n",
    "    # ## CREATION OF YOUR MODEL\n",
    "    UEncK3 = UNetEncoderK3()\n",
    "    UDecK3 = UNetDecoderK3()\n",
    "    UEncK5 = UNetEncoderK5()\n",
    "    UDecK5 = UNetDecoderK5()\n",
    "\n",
    "    print(\n",
    "        \"Total params: {0:,}\".format(\n",
    "            sum(\n",
    "                p.numel()\n",
    "                for p in list(UEncK3.parameters())\n",
    "                + list(UDecK3.parameters())\n",
    "                + list(UEncK5.parameters())\n",
    "                + list(UDecK5.parameters())\n",
    "                if p.requires_grad\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    ce_loss = CrossEntropyLoss()\n",
    "    dice_loss = DiceLoss(4)\n",
    "    softMax = torch.nn.Softmax()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        UEncK3.cuda()\n",
    "        UDecK3.cuda()\n",
    "        UEncK5.cuda()\n",
    "        UDecK5.cuda()\n",
    "        ce_loss.cuda()\n",
    "        dice_loss.cuda()\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    optimizerK3 = optim.SGD(\n",
    "        list(UEncK3.parameters()) + list(UDecK3.parameters()),\n",
    "        lr=base_lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0001,\n",
    "    )\n",
    "    optimizerK5 = optim.SGD(\n",
    "        list(UEncK3.parameters()) + list(UDecK3.parameters()),\n",
    "        lr=base_lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0001,\n",
    "    )\n",
    "    lr_schedulerK3 = PolynomialLR(\n",
    "        optimizerK3,\n",
    "        total_iters=max_iterations,  # The number of steps that the scheduler decays the learning rate.\n",
    "        power=1,\n",
    "    )  # The power of the polynomial.\n",
    "    lr_schedulerK5 = PolynomialLR(\n",
    "        optimizerK5,\n",
    "        total_iters=max_iterations,  # The number of steps that the scheduler decays the learning rate.\n",
    "        power=1,\n",
    "    )  # The power of the polynomial.\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    lossTotalValidation = []\n",
    "    Best_loss_val = 1000\n",
    "    BestEpoch = 0\n",
    "    no_improvement_counter = 0\n",
    "\n",
    "    directory = \"Results/Statistics/\" + \"CrossTeachingK3K5\"\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory) == False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    iter_num = 0\n",
    "    max_epoch = max_iterations // len(trainloader) + 1\n",
    "    print(\"{} iterations per epoch\".format(len(trainloader)))\n",
    "    ## START THE TRAINING\n",
    "    ## FOR EACH EPOCH\n",
    "    for epoch_num in range(max_epoch):\n",
    "        UEncK3.train()\n",
    "        UDecK3.train()\n",
    "        UEncK5.train()\n",
    "        UDecK5.train()\n",
    "        lossEpoch = []\n",
    "        num_batches = len(trainloader)\n",
    "        ## FOR EACH BATCH\n",
    "        for i_batch, sampled_batch in enumerate(trainloader):\n",
    "            ### Set to zero all the gradients\n",
    "            optimizerK3.zero_grad()\n",
    "            optimizerK5.zero_grad()\n",
    "\n",
    "            ## GET IMAGES, LABELS and IMG NAMES\n",
    "            volume_batch, label_batch = sampled_batch[\"image\"], sampled_batch[\"label\"]\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda()\n",
    "\n",
    "            ################### Train ###################\n",
    "            # -- The CNN makes its predictions (forward pass)\n",
    "            featuresK3 = UEncK3(volume_batch)\n",
    "            outK3 = UDecK3(featuresK3)\n",
    "            outK3_soft = torch.softmax(outK3, dim=1)\n",
    "            ##\n",
    "            featuresK5 = UEncK5(volume_batch)\n",
    "            outK5 = UDecK5(featuresK5)\n",
    "            outK5_soft = torch.softmax(outK5, dim=1)\n",
    "\n",
    "            # COMPUTE THE LOSS #adapted from https://github.com/HiLab-git/SSL4MIS/blob/master/code/networks/unet.py\n",
    "            superv_ce_lossK3 = ce_loss(outK3[:8], label_batch.squeeze(1)[:8].long())\n",
    "            superv_dice_lossK3 = dice_loss(outK3_soft[:8], label_batch[:8])\n",
    "            supervised_lossK3 = 0.5 * (superv_ce_lossK3 + superv_dice_lossK3)\n",
    "            ##\n",
    "            superv_ce_lossK5 = ce_loss(outK5[:8], label_batch[:8].squeeze(1).long())\n",
    "            superv_dice_lossK5 = dice_loss(outK5_soft[:8], label_batch[:8])\n",
    "            supervised_lossK5 = 0.5 * (superv_ce_lossK5 + superv_dice_lossK5)\n",
    "\n",
    "            pseudo_lblK3 = torch.argmax(outK3_soft[8:].detach(), dim=1, keepdim=False)\n",
    "            pseudo_lblK5 = torch.argmax(outK5_soft[8:].detach(), dim=1, keepdim=False)\n",
    "\n",
    "            pseudo_suprv_lossK3 = dice_loss(outK3_soft[8:], pseudo_lblK3.unsqueeze(1))\n",
    "            pseudo_suprv_lossK5 = dice_loss(outK5_soft[8:], pseudo_lblK5.unsqueeze(1))\n",
    "\n",
    "            consistency_weight = get_current_consistency_weight(iter_num // 150)\n",
    "\n",
    "            K3Loss = supervised_lossK3 + consistency_weight * pseudo_suprv_lossK3\n",
    "            K5Loss = supervised_lossK5 + consistency_weight * pseudo_suprv_lossK5\n",
    "\n",
    "            loss = K3Loss + K5Loss\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizerK3.step()\n",
    "            optimizerK5.step()\n",
    "            lr_schedulerK3.step()\n",
    "            lr_schedulerK5.step()\n",
    "\n",
    "            iter_num = iter_num + 1\n",
    "\n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING\n",
    "            lossEpoch.append(loss.cpu().data.numpy())\n",
    "            printProgressBar(\n",
    "                i_batch + 1,\n",
    "                num_batches,\n",
    "                prefix=\"[Training] Epoch: {} \".format(epoch_num),\n",
    "                length=15,\n",
    "                suffix=\" Loss: {:.4f}, \".format(loss),\n",
    "            )\n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "\n",
    "        printProgressBar(\n",
    "            num_batches,\n",
    "            num_batches,\n",
    "            done=\"[Training] Epoch: {}, LossG: {:.4f}\".format(epoch_num, lossEpoch),\n",
    "        )\n",
    "        # VALIDATION\n",
    "        UEncK3.eval()\n",
    "        UDecK3.eval()\n",
    "        UEncK5.eval()\n",
    "        UDecK5.eval()\n",
    "        lossEpochVal = []\n",
    "        lossEpochVal1 = []\n",
    "        lossEpochVal2 = []\n",
    "        num_batches = len(val_loader)\n",
    "        for i_batch, sampled_batch in enumerate(val_loader):\n",
    "            images, labels = sampled_batch[\"image\"], sampled_batch[\"label\"]\n",
    "            labels = to_var(labels)\n",
    "            images = to_var(images)\n",
    "\n",
    "            featuresK3 = UEncK3(images)\n",
    "            outK3 = UDecK3(featuresK3)\n",
    "\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            CE_loss_value = ce_loss(outK3, segmentation_classes)\n",
    "\n",
    "            predsoft = softMax(outK3)\n",
    "            pred = predsoft.argmax(dim=1)\n",
    "\n",
    "            # Show live view of model segmentation\n",
    "            if not os.path.exists(\"Results/Segmentation1/\"):\n",
    "                os.makedirs(\"Results/Segmentation1/\")\n",
    "            save_image(\n",
    "                torch.cat([pred.view(labels.shape[0], 1, 256, 256).data / 3.0]),\n",
    "                \"Results/Segmentation1/liveview.png\".format(epoch_num),\n",
    "            )\n",
    "\n",
    "            # Dice_loss_value = computeDSC(pred.unsqueeze(1), segmentation_classes.unsqueeze(1))\n",
    "            Dice_loss_value = dice_loss(predsoft, segmentation_classes.unsqueeze(1))\n",
    "\n",
    "            lossTotal = CE_loss_value + Dice_loss_value\n",
    "\n",
    "            lossEpochVal1.append(lossTotal.cpu().data.numpy())\n",
    "            printProgressBar(\n",
    "                i_batch + 1,\n",
    "                num_batches,\n",
    "                prefix=\"[Validation] Epoch: {} \".format(epoch_num),\n",
    "                length=15,\n",
    "                suffix=\" Loss: {:.4f}, CE: {:.4f}, Dice: {:.8f}\".format(\n",
    "                    lossTotal, CE_loss_value, Dice_loss_value\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # SECOND\n",
    "\n",
    "            featuresK5 = UEncK5(images)\n",
    "            outK5 = UDecK5(featuresK5)\n",
    "\n",
    "            segmentation_classes = getTargetSegmentation(labels)\n",
    "            CE_loss_value = ce_loss(outK5, segmentation_classes)\n",
    "\n",
    "            predsoft = softMax(outK5)\n",
    "            pred = predsoft.argmax(dim=1)\n",
    "\n",
    "            # Show live view of model segmentation\n",
    "            if not os.path.exists(\"Results/Segmentation2/\"):\n",
    "                os.makedirs(\"Results/Segmentation2/\")\n",
    "            save_image(\n",
    "                torch.cat([pred.view(labels.shape[0], 1, 256, 256).data / 3.0]),\n",
    "                \"Results/Segmentation2/liveview.png\".format(epoch_num),\n",
    "            )\n",
    "\n",
    "            # Dice_loss_value = computeDSC(pred.unsqueeze(1), segmentation_classes.unsqueeze(1))\n",
    "            Dice_loss_value = dice_loss(predsoft, segmentation_classes.unsqueeze(1))\n",
    "\n",
    "            lossTotal = CE_loss_value + Dice_loss_value\n",
    "\n",
    "            lossEpochVal2.append(lossTotal.cpu().data.numpy())\n",
    "            printProgressBar(\n",
    "                i_batch + 1,\n",
    "                num_batches,\n",
    "                prefix=\"[Validation] Epoch: {} \".format(epoch_num),\n",
    "                length=15,\n",
    "                suffix=\" Loss: {:.4f}, CE: {:.4f}, Dice: {:.8f}\".format(\n",
    "                    lossTotal, CE_loss_value, Dice_loss_value\n",
    "                ),\n",
    "            )\n",
    "        # Save the model if it is the best so far\n",
    "        modelName = modelName1\n",
    "        lossEpochVal1 = np.asarray(lossEpochVal1)\n",
    "        lossEpochVal1 = lossEpochVal1.mean()\n",
    "        lossEpochVal2 = np.asarray(lossEpochVal2)\n",
    "        lossEpochVal2 = lossEpochVal2.mean()\n",
    "        lossEpochVal = lossEpochVal1\n",
    "        modelStateDict = {\"ENC\": UEncK3.state_dict(), \"DEC\": UDecK3.state_dict()}\n",
    "        print(\"K3: {} K5: {}\".format(lossEpochVal1, lossEpochVal2))\n",
    "        if lossEpochVal2 < lossEpochVal1:\n",
    "            modelName = modelName2\n",
    "            lossEpochVal = lossEpochVal2\n",
    "            modelStateDict = {\"ENC\": UEncK5.state_dict(), \"DEC\": UDecK5.state_dict()}\n",
    "\n",
    "        lossEpochVal = np.asarray(lossEpochVal)\n",
    "        lossEpochVal = lossEpochVal.mean()\n",
    "\n",
    "        lossTotalValidation.append(lossEpochVal)\n",
    "\n",
    "        printProgressBar(\n",
    "            num_batches,\n",
    "            num_batches,\n",
    "            done=\"[Validation] Epoch: {}, LossG: {:.4f}\".format(epoch_num, lossEpochVal),\n",
    "        )\n",
    "\n",
    "        if lossEpochVal < Best_loss_val:\n",
    "            Best_loss_val = lossEpochVal\n",
    "            BestEpoch = epoch_num\n",
    "            no_improvement_counter = 0\n",
    "\n",
    "            if not os.path.exists(\"./models/\" + modelName):\n",
    "                os.makedirs(\"./models/\" + modelName)\n",
    "            torch.save(modelStateDict, \"./models/\" + modelName + \"/\" + str(epoch_num) + \"_Epoch\")\n",
    "            print(\"Best model saved at epoch {}\".format(epoch_num))\n",
    "        else:\n",
    "            no_improvement_counter = no_improvement_counter + 1\n",
    "            print(\n",
    "                \"No improvement in last epoch. Counter: {}\".format(no_improvement_counter))\n",
    "            \n",
    "            if no_improvement_counter % 3 == 0 and no_improvement_counter != 0:\n",
    "                print(\"No improvement in last 3 epochs.\")\n",
    "                new_lr1 = lr_schedulerK3.get_lr()\n",
    "                new_lr2 = lr_schedulerK5.get_lr()\n",
    "                print(new_lr1.shape())\n",
    "                new_lr1 = sum(new_lr1)/len(new_lr1)/10\n",
    "                new_lr2 = sum(new_lr2)/len(new_lr2)/10\n",
    "                optimizerK3 = optim.SGD(\n",
    "                    list(UEncK3.parameters()) + list(UDecK3.parameters()),\n",
    "                    lr=new_lr1,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=0.0001,\n",
    "                )\n",
    "                optimizerK5 = optim.SGD(\n",
    "                    list(UEncK3.parameters()) + list(UDecK3.parameters()),\n",
    "                    lr=new_lr2,\n",
    "                    momentum=0.9,\n",
    "                    weight_decay=0.0001,\n",
    "                )\n",
    "                lr_schedulerK3 = PolynomialLR(\n",
    "                    optimizerK3,\n",
    "                    total_iters=max_iterations,  # The number of steps that the scheduler decays the learning rate.\n",
    "                    power=1,\n",
    "                )  # The power of the polynomial.\n",
    "                lr_schedulerK5 = PolynomialLR(\n",
    "                    optimizerK5,\n",
    "                    total_iters=max_iterations,  # The number of steps that the scheduler decays the learning rate.\n",
    "                    power=1,\n",
    "                )  # The power of the polynomial.\n",
    "\n",
    "            if epoch_num - BestEpoch > 7:\n",
    "                print(\"No improvement in last 7 epochs. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    np.save(os.path.join(directory, \"Losses.npy\"), lossTotalTraining)\n",
    "    np.save(os.path.join(directory, \"Losses_val.npy\"), lossTotalValidation)\n",
    "\n",
    "    print(\"Training finished. Best model saved at epoch {}\".format(BestEpoch))\n",
    "\n",
    "    graphLosses(lossTotalTraining, lossTotalValidation, modelName, directory)\n",
    "\n",
    "    return BestEpoch\n",
    "\n",
    "    # ## THIS IS HOW YOU WILL SAVE THE TRAINED MODELS AFTER EACH EPOCH.\n",
    "    #     ## WARNING!!!!! YOU DON'T WANT TO SAVE IT AT EACH EPOCH, BUT ONLY WHEN THE MODEL WORKS BEST ON THE VALIDATION SET!!\n",
    "    #     if not os.path.exists('./models/' + modelName):\n",
    "    #             os.makedirs('./models/' + modelName)\n",
    "\n",
    "    #         torch.save(net.state_dict(), './models/' + modelName + '/' + str(i) + '_Epoch')\n",
    "\n",
    "    #     np.save(os.path.join(directory, 'Losses.npy'), lossTotalTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f81713-4c18-4dd3-bc72-35dd8f30a339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "~~~~~~~~  Starting the training... ~~~~~~\n",
      "----------------------------------------\n",
      "Total silices is: 1208, labeled slices is: 204\n",
      "~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\n",
      " Model Name1: K3\n",
      " Model Name2: K5\n",
      "Total params: 6,766,344\n",
      "~~~~~~~~~~~ Starting the training ~~~~~~~~~~\n",
      "25 iterations per epoch\n",
      "[Training] Epoch: 0 [DONE]                                 \n",
      "[Training] Epoch: 0, LossG: 1.6607                                                                           \n",
      "[Validation] Epoch: 0 [DONE]                                                             \n",
      "[Validation] Epoch: 0 [DONE]                                                             \n",
      "K3: 0.9222465753555298 K5: 2.238131284713745\n",
      "[Validation] Epoch: 0, LossG: 0.9222                                                                         \n",
      "Best model saved at epoch 0\n",
      "[Training] Epoch: 1 [DONE]                                 \n",
      "[Training] Epoch: 1, LossG: 1.5480                                                                           \n",
      "[Validation] Epoch: 1 [DONE]                                                             \n",
      "[Validation] Epoch: 1 [DONE]                                                             \n",
      "K3: 0.8653819561004639 K5: 2.246001720428467\n",
      "[Validation] Epoch: 1, LossG: 0.8654                                                                         \n",
      "Best model saved at epoch 1\n",
      "[Training] Epoch: 2 [DONE]                                 \n",
      "[Training] Epoch: 2, LossG: 1.5250                                                                           \n",
      "[Validation] Epoch: 2 [DONE]                                                             \n",
      "[Validation] Epoch: 2 [DONE]                                                             \n",
      "K3: 1.0906827449798584 K5: 2.266005039215088\n",
      "[Validation] Epoch: 2, LossG: 1.0907                                                                         \n",
      "No improvement in last epoch. Counter: 1\n",
      "[Training] Epoch: 3 [DONE]                                 \n",
      "[Training] Epoch: 3, LossG: 1.4740                                                                           \n",
      "[Validation] Epoch: 3 [DONE]                                                             \n",
      "[Validation] Epoch: 3 [DONE]                                                             \n",
      "K3: 1.028768539428711 K5: 2.2721424102783203\n",
      "[Validation] Epoch: 3, LossG: 1.0288                                                                         \n",
      "No improvement in last epoch. Counter: 2\n",
      "[Training] Epoch: 4 [DONE]                                 \n",
      "[Training] Epoch: 4, LossG: 1.4505                                                                           \n",
      "[Validation] Epoch: 4 [DONE]                                                             \n",
      "[Validation] Epoch: 4 [DONE]                                                             \n",
      "K3: 1.0136927366256714 K5: 2.2725932598114014\n",
      "[Validation] Epoch: 4, LossG: 1.0137                                                                         \n",
      "No improvement in last epoch. Counter: 3\n",
      "No improvement in last 3 epochs.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'r_schedulerK3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gheith\\OneDrive - ETS\\0 2023 MTI 865 - Apprentissage profind pour la vision par ordinateur\\CleanedGithub\\MTI865-Competition\\Gheith\\Cross_Teaching_CNN_CNN_plus\\mainSegmentationChallenge.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m runTraining()\n",
      "\u001b[1;32mc:\\Users\\gheith\\OneDrive - ETS\\0 2023 MTI 865 - Apprentissage profind pour la vision par ordinateur\\CleanedGithub\\MTI865-Competition\\Gheith\\Cross_Teaching_CNN_CNN_plus\\mainSegmentationChallenge.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=335'>336</a>\u001b[0m \u001b[39mif\u001b[39;00m no_improvement_counter \u001b[39m%\u001b[39m \u001b[39m3\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m no_improvement_counter \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=336'>337</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNo improvement in last 3 epochs.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=337'>338</a>\u001b[0m     new_lr1 \u001b[39m=\u001b[39m r_schedulerK3\u001b[39m.\u001b[39mget_lr()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=338'>339</a>\u001b[0m     new_lr2 \u001b[39m=\u001b[39m lr_schedulerK5\u001b[39m.\u001b[39mget_lr()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=339'>340</a>\u001b[0m     \u001b[39mprint\u001b[39m(new_lr1\u001b[39m.\u001b[39mshape())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r_schedulerK3' is not defined"
     ]
    }
   ],
   "source": [
    "runTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae10184-bacf-4c4d-9767-3272a76a0052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
