{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b07cf2f-b81d-43ef-907a-c49fed2d95ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory:  c:\\Users\\gheith\\OneDrive - ETS\\0 2023 MTI 865 - Apprentissage profind pour la vision par ordinateur\\CleanedGithub\\MTI865-Competition\\Gheith\\Cross_Teaching_CNN_CNN_plus\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "script_path = \"\"\n",
    "try:\n",
    "    os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    for root, dirs, files in os.walk(os.getcwd()):\n",
    "        # Skip 'data' directory and its subdirectories\n",
    "        if \"Data\" in dirs:\n",
    "            dirs.remove(\"Data\")\n",
    "\n",
    "        if \"mainSegmentationChallenge.ipynb\" in files:\n",
    "            script_path = root\n",
    "            break\n",
    "\n",
    "if script_path == \"\":\n",
    "    raise FileNotFoundError(\n",
    "        \"There is a problem in the folder structure.\\nCONTACT gheith.abinader@icloud.com (514)699-5611\"\n",
    "    )\n",
    "\n",
    "os.chdir(script_path)\n",
    "\n",
    "print(\"Current Working Directory: \", os.getcwd())\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from progressBar import printProgressBar\n",
    "\n",
    "import medicalDataLoader\n",
    "import argparse\n",
    "from utils import *\n",
    "\n",
    "from UNet_Base import *\n",
    "import random\n",
    "import torch\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import PolynomialLR\n",
    "from torchvision.utils import save_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e976350-3fd5-4406-8511-86a06a9b4494",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "220c7dcc-8438-454d-97b1-8e989a7b8f06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# put outside of the function for pickeling\n",
    "def worker_init_fn(worker_id):\n",
    "    random.seed(1208 + worker_id)\n",
    "\n",
    "\n",
    "def runTraining():\n",
    "    print(\"-\" * 40)\n",
    "    print(\"~~~~~~~~  Starting the training... ~~~~~~\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    ## DEFINE HYPERPARAMETERS (batch_size > 1)\n",
    "    batch_size = 16\n",
    "    secondaty_batch_size = 8\n",
    "    batch_size_val = 24\n",
    "    base_lr = 0.01  # Learning Rate\n",
    "    max_iterations = 30000\n",
    "\n",
    "    ## DEFINE THE TRANSFORMATIONS TO DO AND THE VARIABLES FOR TRAINING AND VALIDATION\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    mask_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    train_set_full = medicalDataLoader.MedicalImageDataset(\n",
    "        \"train\",\n",
    "        transform=transform,\n",
    "        mask_transform=mask_transform,\n",
    "        augment=False,\n",
    "        equalize=False,\n",
    "    )\n",
    "\n",
    "    total_slices = len(train_set_full)\n",
    "    labeled_slice = 204\n",
    "    print(\n",
    "        \"Total silices is: {}, labeled slices is: {}\".format(\n",
    "            total_slices, labeled_slice\n",
    "        )\n",
    "    )\n",
    "    labeled_idxs = list(range(0, labeled_slice))\n",
    "    unlabeled_idxs = list(range(labeled_slice, total_slices))\n",
    "    batch_sampler = medicalDataLoader.TwoStreamBatchSampler(\n",
    "        labeled_idxs, unlabeled_idxs, batch_size, secondaty_batch_size\n",
    "    )\n",
    "    trainloader = DataLoader(train_set_full, batch_sampler=batch_sampler, num_workers=0)\n",
    "\n",
    "    val_set = medicalDataLoader.MedicalImageDataset(\n",
    "        \"val\", transform=transform, mask_transform=mask_transform, equalize=False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_set,\n",
    "        batch_size=batch_size_val,\n",
    "        worker_init_fn=np.random.seed(0),\n",
    "        num_workers=0,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    ## INITIALIZE YOUR MODEL\n",
    "    print(\"~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\")\n",
    "    modelName1, modelName2 = \"K3\", \"K5\"\n",
    "    print(\" Model Name1: {}\".format(modelName1))\n",
    "    print(\" Model Name2: {}\".format(modelName2))\n",
    "\n",
    "    # ## CREATION OF YOUR MODEL\n",
    "    UEncK3 = UNetEncoderK3()\n",
    "    UDecK3 = UNetDecoderK3()\n",
    "    UEncK5 = UNetEncoderK5()\n",
    "    UDecK5 = UNetDecoderK5()\n",
    "\n",
    "    print(\n",
    "        \"Total params: {0:,}\".format(\n",
    "            sum(\n",
    "                p.numel()\n",
    "                for p in list(UEncK3.parameters())\n",
    "                + list(UDecK3.parameters())\n",
    "                + list(UEncK5.parameters())\n",
    "                + list(UDecK5.parameters())\n",
    "                if p.requires_grad\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # DEFINE YOUR OUTPUT COMPONENTS (e.g., SOFTMAX, LOSS FUNCTION, ETC)\n",
    "    ce_loss = CrossEntropyLoss()\n",
    "    dice_loss = DiceLoss(4)\n",
    "    softMax = torch.nn.Softmax()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        UEncK3.cuda()\n",
    "        UDecK3.cuda()\n",
    "        UEncK5.cuda()\n",
    "        UDecK5.cuda()\n",
    "        ce_loss.cuda()\n",
    "        dice_loss.cuda()\n",
    "\n",
    "    ## DEFINE YOUR OPTIMIZER\n",
    "    optimizerK3 = optim.SGD(\n",
    "        list(UEncK3.parameters()) + list(UDecK3.parameters()),\n",
    "        lr=base_lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0001,\n",
    "    )\n",
    "    optimizerK5 = optim.SGD(\n",
    "        list(UEncK3.parameters()) + list(UDecK3.parameters()),\n",
    "        lr=base_lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=0.0001,\n",
    "    )\n",
    "    lr_schedulerK3 = PolynomialLR(\n",
    "        optimizerK3,\n",
    "        total_iters=max_iterations,  # The number of steps that the scheduler decays the learning rate.\n",
    "        power=1,\n",
    "    )  # The power of the polynomial.\n",
    "    lr_schedulerK5 = PolynomialLR(\n",
    "        optimizerK5,\n",
    "        total_iters=max_iterations,  # The number of steps that the scheduler decays the learning rate.\n",
    "        power=1,\n",
    "    )  # The power of the polynomial.\n",
    "\n",
    "    ### To save statistics ####\n",
    "    lossTotalTraining = []\n",
    "    lossTotalValidation1 = []\n",
    "    lossTotalValidation2 = []\n",
    "    Best_loss_val1 = 1000\n",
    "    Best_loss_val2 = 1000\n",
    "    BestEpoch = 0\n",
    "    performance1 = 0\n",
    "    performance2 = 0\n",
    "\n",
    "    directory = \"Results/Statistics/\" + \"CrossTeachingK3K5\"\n",
    "\n",
    "    print(\"~~~~~~~~~~~ Starting the training ~~~~~~~~~~\")\n",
    "    if os.path.exists(directory) == False:\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    iter_num = 0\n",
    "    max_epoch = max_iterations // len(trainloader) + 1\n",
    "    print(\"{} iterations per epoch\".format(len(trainloader)))\n",
    "    ## START THE TRAINING\n",
    "    ## FOR EACH EPOCH\n",
    "    for epoch_num in range(max_epoch):\n",
    "        UEncK3.train()\n",
    "        UDecK3.train()\n",
    "        UEncK5.train()\n",
    "        UDecK5.train()\n",
    "        lossEpoch = []\n",
    "        num_batches = len(trainloader)\n",
    "        ## FOR EACH BATCH\n",
    "        for i_batch, sampled_batch in enumerate(trainloader):\n",
    "            ### Set to zero all the gradients\n",
    "            optimizerK3.zero_grad()\n",
    "            optimizerK5.zero_grad()\n",
    "\n",
    "            ## GET imagesv, LABELS and IMG NAMES\n",
    "            volume_batch, label_batch = sampled_batch[\"image\"], sampled_batch[\"label\"]\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda()\n",
    "\n",
    "            ################### Train ###################\n",
    "            # -- The CNN makes its predictions (forward pass)\n",
    "            featuresK3 = UEncK3(volume_batch)\n",
    "            outK3 = UDecK3(featuresK3)\n",
    "            outK3_soft = torch.softmax(outK3, dim=1)\n",
    "            ##\n",
    "            featuresK5 = UEncK5(volume_batch)\n",
    "            outK5 = UDecK5(featuresK5)\n",
    "            outK5_soft = torch.softmax(outK5, dim=1)\n",
    "\n",
    "            # COMPUTE THE LOSS #adapted from https://github.com/HiLab-git/SSL4MIS/blob/master/code/networks/unet.py\n",
    "            superv_ce_lossK3 = ce_loss(outK3[:8], label_batch.squeeze(1)[:8].long())\n",
    "            superv_dice_lossK3 = dice_loss(outK3_soft[:8], label_batch[:8])\n",
    "            supervised_lossK3 = 0.5 * (superv_ce_lossK3 + superv_dice_lossK3)\n",
    "            ##\n",
    "            superv_ce_lossK5 = ce_loss(outK5[:8], label_batch[:8].squeeze(1).long())\n",
    "            superv_dice_lossK5 = dice_loss(outK5_soft[:8], label_batch[:8])\n",
    "            supervised_lossK5 = 0.5 * (superv_ce_lossK5 + superv_dice_lossK5)\n",
    "\n",
    "            pseudo_lblK3 = torch.argmax(outK3_soft[8:].detach(), dim=1, keepdim=False)\n",
    "            pseudo_lblK5 = torch.argmax(outK5_soft[8:].detach(), dim=1, keepdim=False)\n",
    "\n",
    "            pseudo_suprv_lossK3 = dice_loss(outK3_soft[8:], pseudo_lblK3.unsqueeze(1))\n",
    "            pseudo_suprv_lossK5 = dice_loss(outK5_soft[8:], pseudo_lblK5.unsqueeze(1))\n",
    "\n",
    "            consistency_weight = get_current_consistency_weight(iter_num // 150)\n",
    "\n",
    "            K3Loss = supervised_lossK3 + consistency_weight * pseudo_suprv_lossK3\n",
    "            K5Loss = supervised_lossK5 + consistency_weight * pseudo_suprv_lossK5\n",
    "\n",
    "            loss = K3Loss + K5Loss\n",
    "            # DO THE STEPS FOR BACKPROP (two things to be done in pytorch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizerK3.step()\n",
    "            optimizerK5.step()\n",
    "            lr_schedulerK3.step()\n",
    "            lr_schedulerK5.step()\n",
    "\n",
    "            iter_num = iter_num + 1\n",
    "\n",
    "            # THIS IS JUST TO VISUALIZE THE TRAINING\n",
    "            lossEpoch.append(loss.cpu().data.numpy())\n",
    "            printProgressBar(\n",
    "                i_batch + 1,\n",
    "                num_batches,\n",
    "                prefix=\"[Training] Epoch: {} \".format(epoch_num),\n",
    "                length=15,\n",
    "                suffix=\" Loss: {:.4f}, \".format(loss),\n",
    "            )\n",
    "            #from the https://github.com/HiLab-git/SSL4MIS/val_2d.py\n",
    "            # EVALUATE BOTH MODELS AND PICK THE BEST ONE\n",
    "            if i_batch>0 and i_batch%24 == 0:\n",
    "                UEncK3.eval()\n",
    "                UDecK3.eval()\n",
    "                UEncK5.eval()\n",
    "                UDecK5.eval()\n",
    "                metric_list_accum1 = 0.0\n",
    "                metric_list_accum2 = 0.0\n",
    "                total_batches = len(val_loader)    \n",
    "                print(\"{} iterations per validaion epoch\".format(total_batches))\n",
    "                random_batch_index = random.randint(0, total_batches - 1)\n",
    "                for i_batch2, sampled_batchv in enumerate(val_loader):\n",
    "                    volume_batchv, label_batchv = sampled_batchv[\"image\"], sampled_batchv[\"label\"]\n",
    "                    imagesv, labelsv = volume_batchv.squeeze(1).cpu().detach().numpy(), label_batchv.squeeze(1).cpu().detach()\n",
    "                    gt_classes = getTargetSegmentation(labelsv).numpy()\n",
    "                    labelsv = labelsv.numpy()\n",
    "                    prediction1 = np.zeros_like(labelsv)\n",
    "                    prediction2 = np.zeros_like(labelsv)\n",
    "                    for ind in range(imagesv.shape[0]):\n",
    "                        slice = imagesv[ind, :, :]\n",
    "                        x, y = slice.shape[0], slice.shape[1]\n",
    "                        input = torch.from_numpy(slice).unsqueeze(0).unsqueeze(0).float().cuda()\n",
    "                        with torch.no_grad():\n",
    "                            featuresK3 = UEncK3(input)\n",
    "                            outK3 = UDecK3(featuresK3)\n",
    "                            out1 = torch.argmax(torch.softmax(outK3, dim=1), dim=1).squeeze(0)\n",
    "                            pred1 = out1.cpu().detach().numpy()\n",
    "                            prediction1[ind] = pred1\n",
    "                            featuresK5 = UEncK5(input)\n",
    "                            outK5 = UDecK5(featuresK5)\n",
    "                            out2 = torch.argmax(torch.softmax(outK5, dim=1), dim=1).squeeze(0)\n",
    "                            pred2 = out2.cpu().detach().numpy()\n",
    "                            prediction2[ind] = pred2\n",
    "                    metric_list1=[]\n",
    "                    metric_list2=[]\n",
    "                    for i in range(1, 4):\n",
    "                        pred1 = np.where(prediction1 == i, 1, 0)\n",
    "                        pred2 = np.where(prediction2 == i, 1, 0)\n",
    "                        gt = np.where(gt_classes == i, 1, 0)\n",
    "                        metric_list1.append(calculate_metric_percase(pred1, gt))\n",
    "                        metric_list2.append(calculate_metric_percase(pred2, gt))\n",
    "                    metric_list_accum1 += np.array(metric_list1)\n",
    "                    metric_list_accum2 += np.array(metric_list2)\n",
    "                    #SAVE IMAGE\n",
    "                    if i_batch%1500 == 0 and ibatch2 == random_batch_index:\n",
    "                        save_sample_imagesv(imagesv, gt_classes, prediction1, predictio2, os.join('Results', 'Segmentation1And2'))\n",
    "                        \n",
    "                    printProgressBar(\n",
    "                            total_batches,\n",
    "                            total_batches,\n",
    "                            done=\"[Validation] Epoch: {}, Loss1: {:.4f}, Loss2: {:.4f}\".format(epoch_num, metric_list_accum1[-1][0], metric_list_accum2[-1][0]),\n",
    "                        )\n",
    "                        \n",
    "                metric_list_accum1 = metric_list_accum1 / len(val_loader)\n",
    "                metric_list_accum2 = metric_list_accum2 / len(val_loader)\n",
    "                for class_i in range(3):\n",
    "                    print('info/model1_val_{}_dice'.format(class_i+1),\n",
    "                                      metric_list_accum1[class_i, 0], iter_num)\n",
    "                    print('info/model1_val_{}_hd95'.format(class_i+1),\n",
    "                                      metric_list_accum1[class_i, 1], iter_num)\n",
    "                    # print('info/model1_val_{}_asd'.format(class_i+1),\n",
    "                    #                   metric_list_accum1[class_i, 2], iter_num)\n",
    "                    print('info/model2_val_{}_dice'.format(class_i+1),\n",
    "                                      metric_list_accum2[class_i, 0], iter_num)\n",
    "                    print('info/model2_val_{}_hd95'.format(class_i+1),\n",
    "                                      metric_list_accum2[class_i, 1], iter_num)\n",
    "                    # print('info/model2_val_{}_asd'.format(class_i+1),\n",
    "                    #                   metric_list_accum2[class_i, 2], iter_num)\n",
    "                \n",
    "                performance1 = np.mean(metric_list_accum1, axis=0)[0]\n",
    "                performance2 = np.mean(metric_list_accum2, axis=0)[0]\n",
    "\n",
    "                # mean_hd951 = np.mean(metric_list_accum1, axis=0)[1]\n",
    "                # mean_hd952 = np.mean(metric_list_accum2, axis=0)[1]\n",
    "                \n",
    "                # mean_asd = np.mean(metric_list_accum1, axis=0)[2]\n",
    "                # mean_asd = np.mean(metric_list_accum2, axis=0)[2]\n",
    "                \n",
    "                print('info/model1_val_mean_dice', performance1, iter_num)\n",
    "                print('info/model2_val_mean_dice', performance2, iter_num)\n",
    "                # print('info/model1_val_mean_hd95', mean_hd951, iter_num)\n",
    "                # print('info/model2_val_mean_hd95', mean_hd952, iter_num)\n",
    "                # print('info/model1_val_mean_asd', mean_asd, iter_num)\n",
    "                # print('info/model2_val_mean_asd', mean_asd, iter_num)\n",
    "                # EXIT\n",
    "                UEncK3.train()\n",
    "                UDecK3.train()\n",
    "                UEncK5.train()\n",
    "                UDecK5.train()\n",
    "                \n",
    "\n",
    "        lossEpoch = np.asarray(lossEpoch)\n",
    "        lossEpoch = lossEpoch.mean()\n",
    "\n",
    "        lossTotalTraining.append(lossEpoch)\n",
    "\n",
    "        printProgressBar(\n",
    "            num_batches,\n",
    "            num_batches,\n",
    "            done=\"[Training] Epoch: {}, LossG: {:.4f}\".format(epoch_num, lossEpoch),\n",
    "        )\n",
    "        # Save the model if it is the best so far\n",
    "        if performance1 > Best_loss_val1:\n",
    "            Best_loss_val1 = performance1\n",
    "            if not os.path.exists(\"./models/\" + modelName1):\n",
    "                os.makedirs(\"./models/\" + modelName1)\n",
    "            torch.save({\"ENC\": UEncK3.state_dict(), \"DEC\": UDecK3.state_dict()}, \"./models/\" + modelName + \"/\" + str(epoch_num) + \"_Epoch\")\n",
    "        if performance2 > Best_loss_val2:\n",
    "            Best_loss_val2 = performance2\n",
    "            if not os.path.exists(\"./models/\" + modelName2):\n",
    "                os.makedirs(\"./models/\" + modelName2)\n",
    "            torch.save({\"ENC\": UEncK5.state_dict(), \"DEC\": UDecK5.state_dict()}, \"./models/\" + modelName + \"/\" + str(epoch_num) + \"_Epoch\")\n",
    "\n",
    "        lossEpochVal1 = performance1\n",
    "        lossEpochVal2 = performance2\n",
    "\n",
    "        lossTotalValidation1.append(lossEpochVal1)\n",
    "        lossTotalValidation2.append(lossEpochVal2)\n",
    "\n",
    "    np.save(os.path.join(directory, \"Losses.npy\"), lossTotalTraining)\n",
    "    np.save(os.path.join(directory, \"Losses_val1.npy\"), lossTotalValidation1)\n",
    "    np.save(os.path.join(directory, \"Losses_val2.npy\"), lossTotalValidation2)\n",
    "\n",
    "    print(\"Training finished. Best model saved at epoch {}\".format(BestEpoch))\n",
    "\n",
    "    graphLosses(lossTotalTraining, lossTotalValidation1, modelName1, directory)\n",
    "    graphLosses(lossTotalTraining, lossTotalValidation2, modelName2, directory)\n",
    "\n",
    "    return Best_loss_val1, Best_loss_val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67f81713-4c18-4dd3-bc72-35dd8f30a339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "~~~~~~~~  Starting the training... ~~~~~~\n",
      "----------------------------------------\n",
      "Total silices is: 1208, labeled slices is: 204\n",
      "~~~~~~~~~~~ Creating the UNet model ~~~~~~~~~~\n",
      " Model Name1: K3\n",
      " Model Name2: K5\n",
      "Total params: 6,766,344\n",
      "~~~~~~~~~~~ Starting the training ~~~~~~~~~~\n",
      "25 iterations per epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training] Epoch: 0 [DONE]                                 \n",
      "4 iterations per validaion epoch\n",
      "[Validation] Epoch: 0, Loss1: 0.0000, Loss2: 0.0016                                                          \n",
      "[Validation] Epoch: 0, Loss1: 0.0000, Loss2: 0.0083                                                          \n",
      "[Validation] Epoch: 0, Loss1: 0.0000, Loss2: 0.0148                                                          \n",
      "[Validation] Epoch: 0, Loss1: 0.0000, Loss2: 0.0148                                                          \n",
      "info/model1_val_1_dice 0.0 25\n",
      "info/model1_val_1_hd95 0.0 25\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\gheith\\OneDrive - ETS\\0 2023 MTI 865 - Apprentissage profind pour la vision par ordinateur\\CleanedGithub\\MTI865-Competition\\Gheith\\Cross_Teaching_CNN_CNN_plus\\mainSegmentationChallenge.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m runTraining()\n",
      "\u001b[1;32mc:\\Users\\gheith\\OneDrive - ETS\\0 2023 MTI 865 - Apprentissage profind pour la vision par ordinateur\\CleanedGithub\\MTI865-Competition\\Gheith\\Cross_Teaching_CNN_CNN_plus\\mainSegmentationChallenge.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=264'>265</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minfo/model1_val_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_dice\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(class_i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m                   metric_list_accum1[class_i, \u001b[39m0\u001b[39m], iter_num)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=266'>267</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minfo/model1_val_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_hd95\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(class_i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m                   metric_list_accum1[class_i, \u001b[39m1\u001b[39m], iter_num)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minfo/model1_val_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_asd\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(class_i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m                   metric_list_accum1[class_i, \u001b[39m2\u001b[39;49m], iter_num)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minfo/model2_val_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_dice\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(class_i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=271'>272</a>\u001b[0m                   metric_list_accum2[class_i, \u001b[39m0\u001b[39m], iter_num)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=272'>273</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minfo/model2_val_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_hd95\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(class_i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/gheith/OneDrive%20-%20ETS/0%202023%20MTI%20865%20-%20Apprentissage%20profind%20pour%20la%20vision%20par%20ordinateur/CleanedGithub/MTI865-Competition/Gheith/Cross_Teaching_CNN_CNN_plus/mainSegmentationChallenge.ipynb#W3sZmlsZQ%3D%3D?line=273'>274</a>\u001b[0m                   metric_list_accum2[class_i, \u001b[39m1\u001b[39m], iter_num)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "runTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae10184-bacf-4c4d-9767-3272a76a0052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
